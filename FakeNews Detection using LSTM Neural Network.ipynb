{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Application of LSTM and GRU Recurrent Neural Networks in Fake NEWS detection"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing necessary libraries \nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport re\nimport numpy as np\nfrom string import punctuation\nfrom zipfile import ZipFile\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing neural network libraries\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/fake-news/train.csv')\ntest = pd.read_csv('../input/fake-news/test.csv')\ntrain_data = train.copy()\ntest_data = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.set_index('id', drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for missing values\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"out of 20,000 training samples, around 40 samples (bothering only the text column) have missing values. so we can drop them at once"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping missing values from text columns alone. \ntrain_data[['title', 'author']] = train_data[['title', 'author']].fillna(value = 'Missing')\ntrain_data = train_data.dropna()\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = []\n[length.append(len(str(text))) for text in train_data['text']]\ntrain_data['length'] = length\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can keep 4500 as max features for training the neural network.\n\n**minimum length is 1 ?? Looks like there are some outliers.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data[train_data['length'] < 50])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are 107 outliers in this dataset. Outliers can be removed. It is a good practice to check the outliers before removing them**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['text'][train_data['length'] < 50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Mostly empty texts. They can be removed since they will surely guide the neural network in the wrong way*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the outliers\ntrain_data = train_data.drop(train_data['text'][train_data['length'] < 50].index, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 4500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing the Text before feeding it into the neural networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the text - converting the words, letters into counts or numbers. \n# We dont need to explicitly remove the punctuations. we have an inbuilt option in Tokenizer for this purpose\ntokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\ntokenizer.fit_on_texts(texts = train_data['text'])\nX = tokenizer.texts_to_sequences(texts = train_data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now applying padding to make them even shaped.\nX = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\ny = train_data['label'].values\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data training data for training and validation.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got our training data preprocessed and ready for training the neural network. \n\nWe have to create a neural network now"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM Neural Network\nlstm_model = Sequential(name = 'lstm_nn_model')\nlstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\nlstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\nlstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\nlstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\nlstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\nlstm_model.add(layer = Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer'))\n# compiling the model\nlstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model_fit = lstm_model.fit(X_train, y_train, epochs = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Constructing GRU Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRU neural Network\ngru_model = Sequential(name = 'gru_nn_model')\ngru_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\ngru_model.add(layer = GRU(units = 120, dropout = 0.2, \n                          recurrent_dropout = 0.2, recurrent_activation = 'relu', \n                          activation = 'relu', name = '2nd_layer'))\ngru_model.add(layer = Dropout(rate = 0.4, name = '3rd_layer'))\ngru_model.add(layer = Dense(units = 120, activation = 'relu', name = '4th_layer'))\ngru_model.add(layer = Dropout(rate = 0.2, name = '5th_layer'))\ngru_model.add(layer = Dense(units = len(set(y)), activation = 'softmax', name = 'output_layer'))\n# compiling the model\ngru_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gru_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gru_model_fit = gru_model.fit(X_train, y_train, epochs = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now preparing the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\ntest_data = test.copy()\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.set_index('id', drop = True)\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Filling the Missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.fillna(' ')\nprint(test_data.shape)\ntest_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(texts = test_data['text'])\ntest_text = tokenizer.texts_to_sequences(texts = test_data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = pad_sequences(sequences = test_text, maxlen = max_features, padding = 'pre')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_prediction = lstm_model.predict_classes(test_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LSTM predictions have more accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':test_data.index, 'label':lstm_prediction})\nsubmission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}